from flask import Flask, render_template, request, redirect, url_for, send_from_directory, flash
import os
import stripe
import mne
from mne.io import RawArray
import numpy as np
import pandas as pd
from asrpy import ASR
from datetime import date
import scipy.stats as stats
from scipy.signal import welch, coherence
import time
from docx import Document
from docx.shared import Pt
from docx.enum.text import WD_PARAGRAPH_ALIGNMENT
import openai
import traceback
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

app = Flask(__name__)
app.config['UPLOAD_FOLDER'] = 'uploads'
app.config['OUTPUT_FOLDER'] = 'outputs'
app.config['SECRET_KEY'] = os.getenv('SECRET_KEY')
app.config['STRIPE_PUBLISHABLE_KEY'] = os.getenv('STRIPE_PUBLISHABLE_KEY')
app.config['STRIPE_SECRET_KEY'] = os.getenv('STRIPE_SECRET_KEY')

stripe.api_key = app.config['STRIPE_SECRET_KEY']

os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)
os.makedirs(app.config['OUTPUT_FOLDER'], exist_ok=True)

def process_eeg_pipeline(file_path, output_dir=None, openai_api_key=None, save_files=True):
    """
    Comprehensive EEG pipeline that processes raw OpenBCI data through:
    1. Reading and filtering
    2. Feature extraction
    3. AI-powered report generation

    Parameters:
    file_path (str): Path to the OpenBCI txt file
    output_dir (str, optional): Directory to save output files. If None, uses current directory.
    openai_api_key (str, optional): OpenAI API key for report generation. If None, no report will be generated.
    save_files (bool): Whether to save intermediate CSV files. If False, only saves the final report.

    Returns:
    dict: Data frames and file paths (if saved) generated by the pipeline
    """
    print("=== Starting Complete EEG Pipeline ===")

    # Always set up output directory even if not saving intermediate files
    if output_dir is None:
        output_dir = os.getcwd()
    os.makedirs(output_dir, exist_ok=True)

    try:
        # Part 1 & 2: Process EEG data and extract features
        print("Step 1: Processing EEG data and extracting features...")
        result_data = process_eeg_data(file_path, output_dir, save_files)

        # Check if feature extraction was successful
        if not result_data:
            print("Error: Feature extraction failed. Pipeline cannot continue.")
            return {}

        # Part 3: Generate report if API key is provided
        if openai_api_key:
            print("Step 2: Generating EEG analysis report...")

            # Always provide output_dir for the report, even if we're not saving CSV files
            report_path = generate_eeg_report_from_data(
                result_data['data_frames'],
                output_dir,  # Always provide the output directory
                openai_api_key
            )

            if report_path:
                result_data['report'] = report_path
                print(f"Complete pipeline finished. Report saved to: {report_path}")
            else:
                print("Warning: Report generation failed, but feature extraction was completed.")
        else:
            print("OpenAI API key not provided. Skipping report generation.")
            print("Pipeline finished with feature extraction only.")

        return result_data
    except Exception as e:
        print(f"Error in pipeline: {str(e)}")
        traceback.print_exc()
        return {}

def convert_docx_to_html(docx_path, html_path):
    doc = Document(docx_path)
    html_content = ""

    for para in doc.paragraphs:
        html_content += f"<p>{para.text}</p>"

    with open(html_path, 'w') as html_file:
        html_file.write(html_content)

def process_eeg_data(file_path, output_dir=None, save_files=True):
    """
    Process EEG data and extract features

    Parameters:
    file_path (str): Path to the OpenBCI txt file
    output_dir (str, optional): Directory to save output files
    save_files (bool): Whether to save the processed data to CSV files

    Returns:
    dict: Contains data frames and file paths (if saved)
    """
    # Get current date for filename
    today = date.today()

    # Dictionary to store results
    result = {
        'data_frames': {
            'filtered': None,
            'temporal': None,
            'spectral': None,
            'coherence': None,
            'correlation': None
        },
        'file_paths': {}
    }

    # PART 1: Read and filter the data
    print("  - Reading and filtering EEG data...")

    # Read data using pandas, skip the header and use comma delimiter
    data = pd.read_csv(file_path, comment='%', header=None, delimiter=',', skiprows=5)

    # Extract EEG channels (columns 1-16, skipping the sample index column)
    eeg_data = data.iloc[:, 1:17].to_numpy().T  # Transpose to get channels x samples

    # Convert from OpenBCI raw values to volts
    scale_factor = (4.5 / (2**23)) / 6.0  # Convert to volts
    eeg_data = eeg_data * scale_factor

    # Set channel names and sampling frequency
    ch_names = ['Fp1', 'Fp2', 'C3', 'C4', 'T5', 'T6', 'O1', 'O2',
                'F7', 'F8', 'F3', 'F4', 'T3', 'T4', 'P3', 'P4']
    sfreq = 125

    # Create channel type list and info object
    ch_types = ['eeg'] * len(ch_names)
    info = mne.create_info(ch_names=ch_names, sfreq=sfreq, ch_types=ch_types)

    # Create raw object
    raw = RawArray(eeg_data, info)

    # Add montage (assuming standard 10-20 system)
    montage = mne.channels.make_standard_montage('standard_1020')
    raw.set_montage(montage, match_case=False, on_missing='warn')

    # Apply basic preprocessing
    raw.filter(l_freq=0.5, h_freq=40, fir_window='blackman', fir_design='firwin')
    raw.notch_filter(60, fir_window='blackman', fir_design='firwin')

    # Apply the ASR
    asr = ASR(sfreq=raw.info["sfreq"], cutoff=30)
    asr.fit(raw)
    raw_asr = asr.transform(raw)

    # Create filtered EEG DataFrame
    raw_asr_data = raw_asr.get_data()
    eeg_df = pd.DataFrame(raw_asr_data.T, columns=raw_asr.ch_names)

    # Store filtered data in result
    result['data_frames']['filtered'] = eeg_df

    # Save processed data to CSV if requested
    if save_files:
        filtered_csv_path = os.path.join(output_dir, f'filtered_eeg_{today}.csv')
        eeg_df.to_csv(filtered_csv_path, index=False)
        result['file_paths']['filtered_data'] = filtered_csv_path
        print(f"  - Filtered EEG data saved to: {filtered_csv_path}")
    else:
        print("  - Filtered EEG data processed (not saved to file)")

    # PART 2: Feature extraction
    print("  - Extracting EEG features...")

    # Helper functions for feature extraction
    def calculate_zero_crossing_rate(data):
        """Calculate zero crossing rate"""
        centered_data = data - np.mean(data)
        zero_crossings = np.where(np.diff(np.signbit(centered_data)))[0]
        return len(zero_crossings) / len(data)

    def calculate_band_power(f, psd, band_range):
        """Calculate power in specific frequency band"""
        low, high = band_range
        mask = (f >= low) & (f <= high)
        return np.trapz(psd[mask], f[mask])

    # Calculate window length for Welch's method
    window_length = 4  # seconds
    nperseg = min(window_length * sfreq, len(eeg_df))
    nperseg = 2**int(np.log2(nperseg))  # Ensure nperseg is power of 2 for efficient FFT

    # Define frequency bands
    freq_bands = {
        'Delta': (1, 4),
        'Theta': (4, 8),
        'Alpha': (8, 13),
        'Beta': (13, 30),
        'Gamma': (30, 100)
    }

    # 1. Extract temporal features
    print("    * Extracting temporal features...")
    temporal_features = {}
    for channel in eeg_df.columns:
        data = eeg_df[channel]
        temporal_features[channel] = {
            'Mean': np.mean(data),
            'Median': np.median(data),
            'Standard Deviation': np.std(data),
            'RMS': np.sqrt(np.mean(data**2)),
            'Zero Cross Rate': calculate_zero_crossing_rate(data),
            'Skewness': stats.skew(data),
            'Kurtosis': stats.kurtosis(data),
            'Peak to Peak': np.ptp(data),
            'Absolute Mean': np.mean(np.abs(data)),
            'Variance': np.var(data)
        }

    # Create temporal features DataFrame
    temporal_df = pd.DataFrame(temporal_features).T

    # Store in result
    result['data_frames']['temporal'] = temporal_df

    # Save if requested
    if save_files:
        temporal_csv_path = os.path.join(output_dir, 'temporal_features.csv')
        temporal_df.to_csv(temporal_csv_path)
        result['file_paths']['temporal_features'] = temporal_csv_path

    # 2. Extract spectral features
    print("    * Extracting spectral features...")
    spectral_features = {}

    for channel in eeg_df.columns:
        data = eeg_df[channel].to_numpy()
        f, psd = welch(data, fs=sfreq, nperseg=nperseg,
                      noverlap=nperseg//2, scaling='density')

        # Calculate band powers
        spectral_features[channel] = {
            f'{band} Power': calculate_band_power(f, psd, freq_range)
            for band, freq_range in freq_bands.items()
        }

        # Add peak frequency and power
        peak_idx = np.argmax(psd)
        spectral_features[channel].update({
            'Peak Frequency': f[peak_idx],
            'Peak Power': psd[peak_idx]
        })

    # Create spectral features DataFrame
    spectral_df = pd.DataFrame(spectral_features).T

    # Store in result
    result['data_frames']['spectral'] = spectral_df

    # Save if requested
    if save_files:
        spectral_csv_path = os.path.join(output_dir, 'spectral_features.csv')
        spectral_df.to_csv(spectral_csv_path)
        result['file_paths']['spectral_features'] = spectral_csv_path

    # 3. Extract spatial features (coherence and correlation)
    print("    * Extracting spatial features...")

    # Calculate correlation matrix
    corrcoef_matrix = eeg_df.corr()

    # Store in result
    result['data_frames']['correlation'] = corrcoef_matrix

    # Calculate coherence between channel pairs
    coherence_features = {}
    channels = eeg_df.columns

    for i, ch1 in enumerate(channels):
        for j, ch2 in enumerate(channels):
            if i < j:  # Only calculate for unique pairs
                f, Cxy = coherence(eeg_df[ch1], eeg_df[ch2],
                                 fs=sfreq, nperseg=nperseg)
                coherence_features[f'{ch1}-{ch2}'] = np.mean(Cxy)

    # Create coherence DataFrame
    coherence_df = pd.DataFrame(list(coherence_features.items()),
                              columns=['Channel Pair', 'Coherence'])

    # Store in result
    result['data_frames']['coherence'] = coherence_df

    # Save if requested
    if save_files:
        coherence_csv_path = os.path.join(output_dir, 'coherence_features.csv')
        correlation_csv_path = os.path.join(output_dir, 'correlation_coefficients.csv')

        coherence_df.to_csv(coherence_csv_path)
        corrcoef_matrix.to_csv(correlation_csv_path)

        result['file_paths']['coherence_features'] = coherence_csv_path
        result['file_paths']['correlation_coefficients'] = correlation_csv_path

    print("  - Feature extraction complete!")

    return result

def generate_eeg_report_from_data(data_frames, output_dir, api_key=None):
    """
    Generate an EEG analysis report using AI directly from data frames

    Parameters:
    data_frames (dict): Dictionary of DataFrames with the extracted features
    output_dir (str): Directory to save the report.
    api_key (str): OpenAI API key

    Returns:
    str: Path to the generated report, or None if report generation failed
    """
    if not api_key:
        print("No OpenAI API key provided. Cannot generate report.")
        return None

    # Set the OpenAI API key
    openai.api_key = api_key

    # Check if we have any data
    if not data_frames or all(v is None for v in data_frames.values()):
        print("Error: No data available. Cannot generate report.")
        return None

    # Create output directory if it doesn't exist
    os.makedirs(output_dir, exist_ok=True)

    # Define analysis questions
    questions = [
        {
            "question": "Temporal Metrics Analysis",
            "focus": (
                "Analyze patient temporal metrics against normal ranges:\n\n"
                "1. Mean Amplitude (0.01-0.1 µV)\n"
                "2. Variance (0.001-0.1 µV²)\n"
                "3. Standard Deviation (0.01-0.05 µV)\n"
                "4. RMS (0.01-0.1 µV)\n"
                "5. Zero Crossing Rate (0.5-5 crossings/sec)\n"
                "6. Skewness (~0)\n"
                "7. Kurtosis (~3)\n"
                "8. Peak-to-Peak (0.05-0.2 µV)\n\n"
                "Present analysis as:\n"
                "1. Clinical interpretation of values\n"
                "2. Table comparing patient values to norms\n"
                "3. Recommendations based on findings"
            )
        },
        # Other questions remain the same...
    ]

    # Store responses
    responses = []

    # Process each question
    for i, q in enumerate(questions, 1):
        print(f"  - Processing analysis {i} of {len(questions)}...")
        # Skip questions where data is missing
        if 'temporal' in q["question"].lower() and (data_frames.get('temporal') is None):
            print(f"    * Skipping {q['question']} due to missing data")
            responses.append((q["question"], "Analysis could not be performed due to missing data."))
            continue
        if 'spectral' in q["question"].lower() and (data_frames.get('spectral') is None):
            print(f"    * Skipping {q['question']} due to missing data")
            responses.append((q["question"], "Analysis could not be performed due to missing data."))
            continue
        if 'coherence' in q["question"].lower() and (data_frames.get('coherence') is None):
            print(f"    * Skipping {q['question']} due to missing data")
            responses.append((q["question"], "Analysis could not be performed due to missing data."))
            continue
        if 'correlation' in q["question"].lower() and (data_frames.get('correlation') is None):
            print(f"    * Skipping {q['question']} due to missing data")
            responses.append((q["question"], "Analysis could not be performed due to missing data."))
            continue

        prompt = create_analysis_prompt_from_data(q["question"], data_frames, q["focus"])
        response = ask_openai(prompt)
        responses.append((q["question"], response))
        time.sleep(2)  # Brief pause between API calls

    # Generate and save the document
    print("  - Generating final report document...")
    try:
        report_path = save_to_doc(responses, data_frames, output_dir)
        print(f"  - Report successfully saved to: {report_path}")
        return report_path
    except Exception as e:
        print(f"  - Error saving report: {str(e)}")
        traceback.print_exc()
        return None

def create_analysis_prompt_from_data(question, data_frames, specific_focus):
    """Create an analysis prompt for OpenAI using DataFrame data directly."""
    prompt = f"""Analyze this EEG data:

Question: {question}
Focus Areas: {specific_focus}

Requirements:
1. Focus on what the patient's actual values mean for their condition
2. Explain why each value is good or concerning
3. Use tables to compare patient values against normal ranges
4. Provide specific clinical recommendations based on these exact values
5. Use clear formatting:
   - "# " for main sections
   - "## " for subsections
   - "- " for bullet points
   - "**text**" for important findings
"""

    # Add specific narrative focus for temporal metrics
    if "Temporal Metrics" in question:
        prompt += """Clinical Interpretation of Values:

Objective: Transform the technical metric analysis into a cohesive, narrative paragraph that:
- Connects individual metrics into a comprehensive story
- Explains the clinical significance of each value
- Uses full paragraphs instead of bullet points
- Provides context for what the measurements mean
- Highlights the overall implications for brain function

Desired Narrative Style:
- Begin with an overarching interpretation of the temporal metrics
- Weave together different metrics (amplitude, variance, standard deviation, etc.)
- Explain how these values relate to each other and overall brain health
- Use medical professional language that is both precise and accessible
- Conclude with the broader neurological implications of these findings

Key Considerations:
- Normal ranges: Mean Amplitude (0.01-0.1 µV)
- Variance range: (0.001-0.1 µV²)
- Standard Deviation: (0.01-0.05 µV)
- Contextualizing measurements beyond their numeric values
"""

    # Add relevant data summaries based on question type
    if 'temporal' in question.lower() and data_frames.get('temporal') is not None:
        prompt += f"\nTemporal Metrics Summary:\n{data_frames['temporal'].describe()}\n"

    if 'coherence' in question.lower() and data_frames.get('coherence') is not None:
        prompt += f"\nCoherence Data Summary:\n{data_frames['coherence'].describe()}\n"

    if 'correlation' in question.lower() and data_frames.get('correlation') is not None:
        prompt += f"\nCorrelation Matrix Summary:\n{data_frames['correlation'].describe()}\n"

    if 'spectral' in question.lower() and data_frames.get('spectral') is not None:
        prompt += f"\nSpectral Features Summary:\n{data_frames['spectral'].describe()}\n"

    return prompt

def ask_openai(prompt):
    """Send a prompt to OpenAI API."""
    try:
        response = openai.ChatCompletion.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": """You are an expert in analyzing EEG data.
                    1. First provide clear interpretations of what the data means for brain function and patient state
                    2. Support your interpretations with organized data presentation
                    3. Use tables when presenting numerical comparisons
                    4. Structure your response with clear headings using markdown
                    5. Bold important findings and clinical implications
                    6. End with clear, actionable recommendations
                """},
                {"role": "user", "content": prompt}
            ],
            max_tokens=7500
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"Error getting response from OpenAI: {str(e)}"

def generate_summary(responses):
    """Generate a comprehensive executive summary."""
    summary_prompt = """Create a concise, narrative-style executive summary of the EEG analysis.
    Focus on providing a clear, accessible overview of the patient's brain function,
    highlighting key findings and their clinical significance.

    Specific guidelines:
    1. Write in a professional, medical narrative style
    2. Use paragraph format instead of bullet points
    3. Synthesize findings across all analysis types
    4. Explain the overall implications of the data
    5. Provide context for what the findings mean for the patient's health
    6. Avoid technical jargon where possible
    7. Conclude with key recommendations

    Include a structured overview addressing:
    - Overall brain state and functionality
    - Significant patterns or anomalies
    - Potential clinical implications
    - Recommended next steps

    Analysis data:
    """

    # Add each analysis section to the prompt
    for question, response in responses:
        summary_prompt += f"\n{question}:\n{response}\n"

    try:
        summary_response = openai.ChatCompletion.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": """You are an expert medical writer specializing in
                neurological analysis. Create a comprehensive yet accessible summary that:
                - Translates complex EEG data into clear, meaningful insights
                - Provides a holistic view of the patient's neurological state
                - Uses professional medical language that is understandable to both
                  medical professionals and patients
                - Focuses on the most significant and actionable findings"""},
                {"role": "user", "content": summary_prompt}
            ],
            max_tokens=1000
        )
        return summary_response.choices[0].message.content
    except Exception as e:
        return f"Comprehensive EEG Analysis Summary could not be generated due to: {str(e)}"

def save_to_doc(responses, data_summary, output_dir):
    """Save responses and data summary to a Word document."""
    doc = Document()

    # Add title
    title = doc.add_heading('EEG Analysis Report', 0)
    title.alignment = WD_PARAGRAPH_ALIGNMENT.CENTER

    # Add timestamp
    timestamp_para = doc.add_paragraph(f"Analysis performed on: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    timestamp_para.alignment = WD_PARAGRAPH_ALIGNMENT.CENTER

    # Generate and add executive summary
    doc.add_heading('Executive Summary', 1)
    summary = generate_summary(responses)

    # Parse summary with markdown-like formatting
    parse_markdown_to_docx(doc, summary)

    # Add data summary
    doc.add_heading('Data Sources', 1)

    # Create table for data sources
    data_types = ['temporal', 'spectral', 'coherence', 'correlation']
    table = doc.add_table(rows=len(data_types) + 1, cols=3)
    table.style = 'Table Grid'

    # Table headers
    headers = ['Data Type', 'Rows', 'Columns']
    for col, header in enumerate(headers):
        cell = table.cell(0, col)
        cell.text = header

        for paragraph in cell.paragraphs:
            for run in paragraph.runs:
                run.bold = True

    # Add data rows
    for row, data_type in enumerate(data_types, start=1):
        df = data_summary.get(data_type)
        if df is not None:
            table.cell(row, 0).text = f"{data_type}_features"
            table.cell(row, 1).text = str(df.shape[0])
            table.cell(row, 2).text = str(df.shape[1])

    # Add analysis results
    for i, (question, response) in enumerate(responses, 1):
        doc.add_heading(f"Analysis {i}: {question}", 1)

        # Parse response with markdown-like formatting
        parse_markdown_to_docx(doc, response)

    # Save with timestamp
    timestamp = time.strftime('%Y%m%d_%H%M%S')
    filename = f'EEG_Analysis_Report_{timestamp}.docx'
    file_path = os.path.join(output_dir, filename)
    doc.save(file_path)
    return file_path

def parse_markdown_to_docx(doc, text):
    """Convert markdown text to Word document formatting."""
    lines = text.split('\n')
    i = 0
    while i < len(lines):
        line = lines[i].strip()

        # Skip empty lines
        if not line:
            i += 1
            continue

        # Handle headings
        if line.startswith('# '):
            doc.add_heading(line[2:].strip().replace('**', ''), level=1)
        elif line.startswith('## '):
            doc.add_heading(line[3:].strip().replace('**', ''), level=2)
        elif line.startswith('### '):
            doc.add_heading(line[4:].strip().replace('**', ''), level=3)
        # Handle bullet points
        elif line.startswith('- '):
            p = doc.add_paragraph(style='List Bullet')
            process_text_with_formatting(p, line[2:].strip())
        # Detect table block by checking for a "|" character
        elif '|' in line and not line.startswith('#'):
            # Gather contiguous lines that form the table block
            table_lines = []
            while i < len(lines) and '|' in lines[i]:
                table_lines.append(lines[i].strip())
                i += 1
            # Process the collected table block only once
            create_table_from_markdown_block(doc, table_lines)
            continue  # skip the normal increment since we've already moved ahead
        else:
            p = doc.add_paragraph()
            process_text_with_formatting(p, line)
        i += 1

def create_table_from_markdown_block(doc, table_lines):
    """Convert a block of markdown table lines into a Word table."""
    if len(table_lines) < 2:
        return  # Not enough lines to form a table

    # Process header row: remove leading/trailing pipes and extra spaces
    headers = [cell.strip().replace('**', '') for cell in table_lines[0].strip('|').split('|')]
    num_columns = len(headers)

    # Create the table: one row for headers plus one row per data line (skip the separator)
    table = doc.add_table(rows=len(table_lines) - 1, cols=num_columns)
    table.style = 'Table Grid'

    # Fill header row and make bold
    for col, header in enumerate(headers):
        cell = table.cell(0, col)
        cell.text = header
        for paragraph in cell.paragraphs:
            for run in paragraph.runs:
                run.bold = True

    # Process data rows (starting from third line, after header and separator)
    for row_index, line in enumerate(table_lines[2:], start=1):
        # Remove leading/trailing pipes and split cells
        row_cells = [cell.strip().replace('**', '') for cell in line.strip('|').split('|')]
        # Pad the row if it has fewer cells than the header
        row_cells += [''] * (num_columns - len(row_cells))
        for col, cell_text in enumerate(row_cells):
            table.cell(row_index, col).text = cell_text

    # Add a paragraph after the table for spacing
    doc.add_paragraph()

def process_text_with_formatting(paragraph, text):
    """Process text with bold formatting indicated by ** markers."""
    parts = text.split('**')
    is_bold = False
    for part in parts:
        run = paragraph.add_run(part)
        run.bold = is_bold
        is_bold = not is_bold

@app.route('/')
def index():
    return render_template('index.html')

@app.route('/upload', methods=['GET', 'POST'])
def upload_file():
    if request.method == 'POST':
        if 'file' not in request.files:
            flash('No file part')
            return redirect(request.url)
        file = request.files['file']
        if file.filename == '':
            flash('No selected file')
            return redirect(request.url)
        if file:
            file_path = os.path.join(app.config['UPLOAD_FOLDER'], file.filename)
            file.save(file_path)

            result = process_eeg_pipeline(file_path, app.config['OUTPUT_FOLDER'], openai_api_key=os.getenv('OPENAI_API_KEY'))

            if result and 'report' in result:
                report_filename = os.path.basename(result['report'])
                return redirect(url_for('payment_endpoint', filename=report_filename))
            else:
                flash('Error generating report')
                return redirect(url_for('index'))
    return render_template('upload.html')

@app.route('/payment/<filename>', methods=['GET'], endpoint='payment_endpoint')
def payment(filename):
    # Create a PaymentIntent and pass the client secret to the frontend
    intent = stripe.PaymentIntent.create(
        amount=2000,  # Amount in cents
        currency='usd',
        payment_method_types=['card'],
    )
    return render_template('payment.html', key=app.config['STRIPE_PUBLISHABLE_KEY'], client_secret=intent.client_secret, filename=filename)

@app.route('/view-report/<filename>', endpoint='view_report_endpoint')
def view_report(filename):
    file_path = os.path.join(app.config['OUTPUT_FOLDER'], filename)
    if not os.path.exists(file_path):
        flash('Report file not found')
        return redirect(url_for('index'))

    # Read the .docx file
    doc = Document(file_path)
    report_content = ""

    for para in doc.paragraphs:
        report_content += f"<p>{para.text}</p>"

    return render_template('view_report.html', filename=filename, content=report_content)

@app.route('/download/<filename>')
def download_file(filename):
    try:
        return send_from_directory(app.config['OUTPUT_FOLDER'], filename, as_attachment=True)
    except FileNotFoundError:
        flash('File not found')
        return redirect(url_for('index'))

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000, debug=True)
